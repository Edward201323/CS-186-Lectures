\section{B+ Trees}

\subsection{Sorted File with Holes}

Start with a heap file, sort it, but leave empty slots in each page.

\textbf{Why leave holes?}
\begin{itemize}
    \item Insertions and deletions no longer require shifting all records.
    \item New records can fill existing holes.
\end{itemize}

\textbf{Trade-off:} More pages are needed to store the same amount of data, since each page is partially empty.

\textbf{Remaining problem:} Binary search still has a fanout of 2 (we can only go left or right at each step). Can we do better?

\subsection{Separating Keys from Data}

Split storage into two types of pages:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/key_datapage.png}
    \caption{Data pages store full records (tuples), while key lookup pages store only the indexed column value and a pointer to the record's location.}
    \label{fig:placeholder}
\end{figure}

\textbf{Why this helps:}
\begin{itemize}
    \item Keys are smaller than full tuples (one column vs.\ many columns).
    \item More keys fit per page $\rightarrow$ fewer pages to search through.
    \item Data pages no longer need to be sorted. The key pages handle navigation.
\end{itemize}

\textbf{What a key page entry stores:}
\begin{enumerate}
    \item The key value (e.g., \texttt{major} or \texttt{nameLast}).
    \item A pointer to where the record lives (page ID, slot ID).
\end{enumerate}

This is analogous to a book index. At the back of a textbook, you might find:

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Topic} & \textbf{Page} \\
        \midrule
        B+ trees & 142\\
        Binary search & 58 \\
        Hash tables & 97 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Key:} The topic name (``B+ trees'')
    \item \textbf{Pointers:} The page numbers (142)
\end{itemize}

You don't store the entire explanation of B+ trees in the index but rather just enough to tell you where to find it. Database indexes work the same way.

\subsection{Tree Structure}

Instead of binary search on key pages, build a tree structure with \textbf{high fanout}.

\begin{itemize}
    \item \textbf{Binary tree:} Fanout of 2 (go left or right).
    \item \textbf{B+ tree:} Fanout of hundreds or thousands (many children per node).
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/B+_tree.png}
    \caption{B+ Tree Structure}
    \label{fig:bplustree}
\end{figure}

\subsubsection*{Sorting Property}

Keys within each node are \textbf{sorted}. When searching for a key, you compare your search value against the keys to decide which pointer to follow.

\textbf{Example:} Searching for 27 in an interior node with keys [5 $|$ 13 $|$ 24]:
\begin{itemize}
    \item Values $\leq 5$: follow leftmost pointer
    \item Values $> 5$ and $\leq 13$: follow second pointer
    \item Values $> 13$ and $\leq 24$: follow third pointer
    \item Values $> 24$: follow rightmost pointer
\end{itemize}

Since $27 > 24$, we follow the rightmost pointer. This process repeats at each level until we reach a leaf.

\subsubsection*{Properties of B+ Trees}

\begin{itemize}
    \item \textbf{Balanced:} All leaf nodes are at the same depth/level, guaranteeing consistent search performance.
    \item \textbf{High fanout:} Each node contains many keys and pointers, reducing the number of levels needed.
    \item \textbf{Data at leaves only:} Interior nodes store keys and pointers to child pages; only leaf nodes store pointers to actual records.
    \item \textbf{Linked leaves:} Leaf pages are doubly linked, enabling efficient range scans (find keys $>= 27$) without traversing back up the tree.
    \item \textbf{Occupancy invariant:} Every node (except the root) must be at least half full. This helps the tree maintain a high fanout, and prevents it from degenerating into a binary tree.
\end{itemize}

\subsubsection*{Why High Fanout Matters}

With a binary tree (fanout 2), searching 1 million records requires $\log_2(1{,}000{,}000) \approx 20$ levels.

With a B+ tree (fanout 2,000), the same search requires $\log_{2000}(1{,}000{,}000) \approx 2$ levels.

Fewer levels means fewer page reads (each level is one page read), which means faster searches.

\textbf{Why high fanout matters:}

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Tree Type} & \textbf{Fanout} & \textbf{Height for 4 million records} \\
        \midrule
        Binary tree & 2 & $\approx 22$ levels \\
        B+ tree & 2,000 & 2 levels \\
        \bottomrule
    \end{tabular}
    \caption{Impact of fanout on tree height}
    \label{tab:fanout}
\end{table}

Disk reads are expensive because data must be loaded from disk into memory. Once data is in memory, we treat computation on it as free.

\textbf{Note:} We don't load all keys onto one page and do binary search because page size is fixed by the hardware. We can't create one giant page containing all the keys, and for large datasets (millions or billions of records), the keys wouldn't fit anyway.

\textbf{Note:} Disk I/O (reading/writing) is expensive because data must be transferred from disk into memory. Computation performed entirely in memory is treated as free.

\subsection{B+ Tree Insertion}

Insertion in a B+ tree always begins at the leaf level. When inserting a key into a B+ tree:
\begin{itemize}
    \item We search down the tree to find the correct leaf node.
    \item Insert the key in sorted order.
    \item If the leaf has room, we are done.
\end{itemize}
Most insertions are straightforward. We simply find the appropriate leaf and place the key in its sorted position. The tree structure only changes when a node overflows.

\textbf{Splitting a Leaf Node when it overflows:}

Each node has a maximum capacity. When a node becomes full and we attempt to insert another key, the node \textbf{overflows} and must be \textbf{split}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/leafInsertionex1.png}
    \caption{61 was attempted to be inserted, but the node was already at its maximum size of 4.}
    \label{fig:placeholder}
\end{figure}

Divide the keys: the smaller half go into the left node, the larger half go into the right node.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/leafInsertionex2.png}
    \caption{61 was attempted to be inserted, but the node was already at its maximum capacity, triggering a split due to the max node size.}
    \label{fig:placeholder}
\end{figure}

Next, we copy up the middle key (smallest key of the right node or largest in the left node) into the parent as a separator.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/leafInsertionex3.png}
    \caption{The middle key, 73, is copied up into the parent node to act as a separator between the left node (61, 67) and the right node (79, 97).}
    \label{fig:placeholder}
\end{figure}

The parent now points to both the left and right nodes, with the copied-up key acting as a \textbf{separator}, where all keys in the left subtree are less than or equal to it, and all keys in the right subtree are greater than it.

\textbf{Splitting an Interior Node:}
 
Divide the keys: the smallest half go into the left node, the largest half go into the right node.

\begin{figure}[H]
    \centering    \includegraphics[width=0.5\linewidth]{images/interiorInsertionex1.png}
    \caption{The full interior node is split: keys are divided so that 17 and 23 move into the left node, while 31 and 37 move into the right node.}
    \label{fig:placeholder}
\end{figure}

Redistribute the child pointers appropriately between the two new nodes.

\textbf{Push up} the middle key into the parent. If the parent is already full, it will overflow, and we repeat the splitting process on the parent with recursion.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/interiorInsertionex2.png}
    \caption{Child pointers are redistributed to match the split, and the middle key, 43, is copied upward from the original node.}
    \label{fig:placeholder}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/interiorInsertionex3.png}
    \caption{The middle key, 43, becomes the new parent node, with the left child containing keys 11 and 29 and the right child containing keys 67 and 89, completing the recursive split.}
    \label{fig:placeholder}
\end{figure}

Splitting is a \textbf{recursive} process. When we split a node and push a key into the parent, the parent may itself overflow. If so, we repeat the splitting process on the parent.

\textbf{Example:} Suppose we insert a key into a full leaf. We split the leaf and push a key into the parent. But the parent was already full, so it now overflows. We split the parent into two nodes and push a key into the grandparent. This process continues up the tree as needed.

\textbf{The tree node grows taller only when the root splits.} In this case:
\begin{itemize}
    \item Split the root into two nodes.
    \item Create a new root node containing the pushed-up middle key.
    \item The new root points to the two nodes created from the old root.
\end{itemize}

This ensures the tree remains \textbf{balanced}, where all leaves stay at the same depth. The tree grows from the root upward, not from the leaves downward.

\subsection{B+ Tree Deletion}

To delete a key, we search for it using the standard search algorithm, remove it from its node, and if the node still has at least the minimum number of keys, we are done.

However, sometimes when we delete a key, it causes a node to have fewer than the minimum number of keys. When this happens, we borrow a key from a node next to it/sibling.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/underflowDeleteex1.png}
    \caption{Deleting key 16 causes the leaf node to underflow, leaving it with fewer than the minimum number of keys.}
    \label{fig:placeholder}
\end{figure}


When borrowing from the right sibling when it has more than the minimum number of keys, we take the smallest key from the right sibling, which becomes the new separator in the parent. The old separator from the parent moves down to fill the gap in the underflowing node.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/underflowDeleteex2.png}
    \caption{The underflowing node borrows from its right sibling: key 21 becomes the new separator in the parent, and the old separator moves down to restore the minimum occupancy.}
    \label{fig:placeholder}
\end{figure}

If we instead borrow from the left sibling, we take the largest key from the left sibling, which becomes the new separator in the parent.

\textbf{Important:} We cannot simply move a key directly from sibling to sibling. The separator key in the parent must be updated to maintain the B+ tree property (all keys to the left of a separator are smaller; all keys to the right are larger).

If neither sibling can spare a key (both are at minimum occupancy), we must \textbf{merge} nodes. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/merge1.png}
    \caption{Neither sibling can spare a key, so the underflowing node and its sibling are selected for merging, along with the separator key from the parent.}
    \label{fig:placeholder}
\end{figure}

We do this by combining the node where a key was deleted, a sibling node, and the separator between them, and merging them into a single full node.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/merge2.png}
    \caption{The nodes are merged into a single node containing keys 18, 20, 21, and 24, and the separator is removed from the parent.}
    \label{fig:placeholder}
\end{figure}

After merging, the parent may itself underflow. If so, we recursively apply the same strategy: first try to borrow from a sibling, and if that fails, merge with a sibling.

The tree becomes shorter when a merge propagates all the way to the root and the root ends up with no keys. In this case:
\begin{itemize}
    \item The root is deleted.
    \item The single remaining child of the old root becomes the new root.
\end{itemize}

This is the only way the tree's height decreases, maintaining the property that all leaves are at the same level.

\textbf{Deleting from Interior Nodes.}

When the key to be deleted is in an interior node (acting as a separator), we need a replacement separator.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/interior1.png}
    \caption{Key 28 is deleted from an interior node, leaving a gap that requires choosing a replacement separator from one of its subtrees.}
    \label{fig:placeholder}
\end{figure}

A valid replacement must be:
\begin{itemize}
    \item Greater than all keys in the left subtree, and
    \item Smaller than all keys in the right subtree.
\end{itemize}

So we must choose either the \textbf{largest key in the left subtree} or the \textbf{smallest key in the right subtree.} We replace the deleted key with the chosen candidate, then delete the candidate from its original leaf node. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/interior2.png}
    \caption{The separator is replaced using the smallest key in the right subtree.}
    \label{fig:placeholder}
\end{figure}

This may trigger underflow handling in that leaf, but we just fix it buy borrowing a key from a sibling or merging with a sibling.

\subsection{Leaf Page Organization}

B+ tree leaf pages can be organized in different ways depending on how they connect to the actual data. This choice affects lookup speed, storage requirements, and how many indexes you can create.

We want multiple B+ trees because a single B+ tree can only speed up queries on the column(s) it's indexed on. For example, if you have a B+ tree on \texttt{player\_id}, a query like \texttt{WHERE name = 'Joe'} can't be used. You'd need a separate B+ tree on \texttt{name}. Since we often want multiple indexes (B+ trees) per table, the way we organize leaf pages matters.


\subsubsection*{Alternative 1: Storing By Value}
\begin{itemize}
    \item Store the actual tuples directly in the leaf pages
    \item One fewer pointer to follow allowing faster lookups
    \item Leaves become much larger since they hold full tuples, not just pointers
    \item You can only have ONE such index (B+ Tree) per table. If you want a second index, you have to duplicate your data.
\end{itemize}

\subsubsection*{Alternative 2: By Reference}
\begin{itemize}
    \item Leaf pages store pointers (page number + record ID) to tuples
    \item This is the standard approach we've been using
    \item Allows multiple indexes on the same table  (each index just stores pointers to the same data)
\end{itemize}

\subsubsection*{Alternative 3: List of References}
\begin{itemize}
    \item Like Alternative 2, but stores a \textit{list} of pointers per key
    \item Useful when multiple records share the same key (e.g., many players named ``Joe'')
    \item To find a specific record among duplicates, you have to follow each pointer and check one by one
\end{itemize}

\subsection{Clustered vs Unclustered}

Clustering refers to whether the data pages themselves are sorted by the index key. This affects how efficiently we can read data, especially for range queries.

\subsubsection*{Clustered Index}
\begin{itemize}
    \item The data pages are \textbf{sorted} by the index key (essentially a sorted file with a B+ tree on top)
    \item Pointers from leaf pages go in parallel, no criss-crossing
    \item \textbf{Pro:} Range queries are fast \texttt{(e.g., WHERE birth\_year > 1970}). Find the starting point and read sequentially 
    \item \textbf{Pro:} Nearby keys are stored on nearby (or the same) pages
    \item \textbf{Con:} Harder to maintain, as Insertions may require shuffling data to keep things sorted
    \item In practice, pages are kept about $\frac{2}{3}$ full to leave room for future inserts
\end{itemize}

\subsubsection*{Unclustered Index}
\begin{itemize}
    \item The data pages are a \textbf{heap} (unsorted)---the B+ tree is built on top of unordered data
    \item Pointers from leaf pages go everywhere
    \item \textbf{Pro:} Easier to maintain. Inserts just go wherever there's space
    \item \textbf{Con:} Range queries are slow. You may have to jump to many different pages
\end{itemize}

\subsubsection*{Why Only One Clustered Index Per Table?}

You can only sort your data \textbf{one way}. Think of it like arranging chairs in a room. You can sort them by age or by color, but not both at the same time unless you duplicate them.

Since a clustered index requires the data pages to be sorted by that index's key, you can only have \textbf{one clustered index per table}. Any additional indexes must be unclustered (they just point to wherever the data happens to be).

\subsubsection*{Sequential vs Random I/O}

This matters because of how disks work:
\begin{itemize}
    \item \textbf{Sequential reads} (clustered): Reading consecutive pages is fast; up to 100$\times$ faster on spinning disks
    \item \textbf{Random reads} (unclustered): Jumping around to different pages is slow, as the disk arm has to move each time
\end{itemize}

For SSDs, this difference is smaller, but sequential reads are still faster.

\subsection{Composite Indexes}

When you create a composite index on multiple columns, the order matters. The data is sorted by the first column, then by the second column as a tiebreaker, and so on.

Suppose we have a B+ tree index on \texttt{(birth\_year, weight)}. Which queries can use the index?

\begin{table}[H]
    \centering
    \begin{tabular}{lcp{7cm}}
        \toprule
        \textbf{Query} & \textbf{Can use index?} & \textbf{Why?} \\
        \midrule
        \texttt{birth\_year = 1970} & Yes & Directly matches the first indexed column \\
        \texttt{weight > 200} & No & Weight is only the tiebreaker. \\
        \texttt{birth\_year = 1970 AND weight > 200} & Yes & Find all 1970 records, then filter by weight (which is sorted within that group) \\
        \texttt{birth\_year > 1970} & Yes & Find the first 1970 record, then read everything to the right \\
        \bottomrule
    \end{tabular}
\end{table}

The first column in a composite index \texttt{(birth\_year, weight)} has more significance. You can only use the index efficiently if your query includes the first column. Queries on just the second column can't use the index.

\subsection{I/O Cost}

We want to compare B+ trees against heap files and sorted files. Remember: we count I/O operations (page reads/writes), not Big-O complexity. Constant factors matter in databases.

\subsubsection*{Key Variables}

\begin{table}[H]
    \centering
    \begin{tabular}{cl}
        \toprule
        \textbf{Variable} & \textbf{Meaning} \\
        \midrule
        $B$ & Number of data pages (if fully packed) \\
        $D$ & Time to read/write one page \\
        $R$ & Records per page \\
        $F$ & Fanout (typically $\approx 2000$, \textbf{not} 2) \\
        $E$ & Entries per leaf page \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection*{Why Fanout Matters}

Both sorted files and B+ trees have logarithmic lookup time, but:
\begin{itemize}
    \item Sorted file (binary search): $\log_2(n)$
    \item B+ tree: $\log_F(n)$ where $F \approx 2000$
\end{itemize}

$\log_{2000}(n)$ is \textbf{much smaller} than $\log_2(n)$. For 1 million records:
\begin{itemize}
    \item Binary search: $\log_2(1{,}000{,}000) \approx 20$ page reads
    \item B+ tree: $\log_{2000}(1{,}000{,}000) \approx 2$ page reads
\end{itemize}

This is why B+ trees beat sorted files---same Big-O, but the constant factor is huge.

\subsubsection*{Cost Formulas}

\textbf{Scan all records:}
\[
\text{Cost} = 1.5 \cdot B \cdot D
\]
The $1.5$ factor comes from pages being $\frac{2}{3}$ full (we leave room for inserts). We don't need the index for this---just read all data pages.

\textbf{Equality search:}
\[
\text{Cost} = \left( \log_F \left( \frac{B \cdot R}{E} \right) + 2 \right) \cdot D
\]
\begin{itemize}
    \item $\log_F(\ldots)$: traverse the tree from root to leaf
    \item $+1$: accounts for reading the root
    \item $+1$: fetch the actual data page (following the pointer)
\end{itemize}

\textbf{Range search:}
\[
\text{Cost} = \left( \log_F \left( \frac{B \cdot R}{E} \right) + \text{num\_pages} \right) \cdot D
\]
Find the starting point, then read all matching pages. In a clustered index, these pages are sequential.

\subsubsection*{Cost Comparison}

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Operation} & \textbf{Heap File} & \textbf{Sorted File} & \textbf{B+ Tree (Clustered)} \\
        \midrule
        Scan & $B$ & $B$ & $1.5B$ \\
        Equality Search & $0.5B$ & $\log_2 B$ & $\log_F B$ \\
        Range Search & $B$ & $\log_2 B + \text{pages}$ & $\log_F B + \text{pages}$ \\
        \bottomrule
    \end{tabular}
    \caption{I/O cost comparison (in page reads, $D$ omitted)}
\end{table}

\subsubsection*{Key Takeaways}

\begin{itemize}
    \item \textbf{Constant factors matter}: $\log_F$ vs $\log_2$ makes a big difference when $F \approx 2000$
    \item \textbf{Sequential I/O $\gg$ Random I/O}: on spinning disks, sequential reads are $\sim$100$\times$ faster
    \item \textbf{Clustered indexes} allow sequential reads for range queries; unclustered indexes require random reads
    \item The $1.5\times$ scan cost comes from leaving pages $\frac{2}{3}$ full for future inserts
\end{itemize}